{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "4Rnf5KnqqjfQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class NaiveDense:\n",
        "  def __init__(self,input_size,output_size,activation):\n",
        "    self.activation=activation\n",
        "\n",
        "    w_shape=(input_size,output_size) #build array W\n",
        "    w_initial_value=tf.random.uniform(w_shape,minval=0,maxval=1e-1) #array initial set to random(0 to 0.1)\n",
        "    self.w=tf.Variable(w_initial_value) #build w Variable\n",
        "\n",
        "    b_shape=(output_size,) #build b Vector\n",
        "    b_inital_value=tf.zeros(b_shape) #make Vetor inital to 0\n",
        "    self.b=tf.Variable(b_inital_value) #build b Variable\n",
        "\n",
        "  def __call__(self,inputs): #build forward-propagation method\n",
        "    return self.activation(tf.matmul(inputs,self.w)+self.b)\n",
        "\n",
        "  @property #set weights only read can't rewrite\n",
        "  def weights(self): #get Desen Weights\n",
        "    return [self.w,self.b]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveSequential:\n",
        "  def __init__(self,layers):\n",
        "    self.layers = layers\n",
        "\n",
        "  def __call__(self,inputs):\n",
        "    x = inputs         #\n",
        "    for layer in self.layers: # inputs data pass to all layers\n",
        "      x = layer(x)      #\n",
        "    return x\n",
        "\n",
        "  @property\n",
        "  def weights(self):\n",
        "    weights = []\n",
        "    for layer in self.layers:\n",
        "      weights += layer.weights\n",
        "    return weights"
      ],
      "metadata": {
        "id": "fjwD3vVHqy6S"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NaiveSequential([\n",
        "    NaiveDense(input_size=28 * 28, output_size=512, activation=tf.nn.relu),\n",
        "    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)\n",
        "])\n",
        "assert len(model.weights) == 4"
      ],
      "metadata": {
        "id": "rhjiCTkt5eks"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class BatchGenerator:\n",
        "  def __init__(self,images,labels,batch_size=128):\n",
        "\n",
        "    assert len(images) ==len(labels) #check images have correct labels\n",
        "    self.index=0\n",
        "    self.images=images\n",
        "    self.labels=labels\n",
        "    self.batch_size=batch_size\n",
        "    self.num_batches=math.ceil(len(images)/batch_size) #account each batch\n",
        "\n",
        "  def next(self):\n",
        "    images=self.images[self.index:self.index+self.batch_size]\n",
        "    labels=self.labels[self.index:self.index+self.batch_size]\n",
        "    self.index+=self.batch_size\n",
        "    return images,labels"
      ],
      "metadata": {
        "id": "5B3mSprWvuuI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_training_step(model,images_batch,labels_batch):\n",
        "  with tf.GradientTape()as tape: #recode forward-propagation accountgraph\n",
        "    predictions = model(images_batch) #input each batch images\n",
        "    per_sample_losses=(tf.keras.losses.sparse_categorical_crossentropy(labels_batch,predictions)) #account each sample loss\n",
        "    average_loss=tf.reduce_mean(per_sample_losses) #average this batch loss\n",
        "\n",
        "  gradients=tape.gradient(average_loss,model.weights)\n",
        "\n",
        "  update_weights(gradients,model.weights)\n",
        "  return average_loss\n",
        "\n",
        "learning_rate=1e-3\n",
        "\n",
        "def update_weights(gradients,weights):\n",
        "  for g,w,in zip(gradients,weights): #update each weights\n",
        "    w.assign_sub(g*learning_rate)"
      ],
      "metadata": {
        "id": "jioFqm8txPYy"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model,images,labels,epochs,batch_size=128): #定義fit訓練迴圈\n",
        "\n",
        "  for epoch_counter in range(epochs): #執行多少epoch次數\n",
        "    print(f\"Epoch {epoch_counter}\")\n",
        "    batch_generator=BatchGenerator(images,labels) #建立小批次訓練資料的產生器\n",
        "\n",
        "    for batch_counter in range(batch_generator.num_batches):\n",
        "      images_batch,labels_batch=batch_generator.next() #取出小批次的訓練資料\n",
        "\n",
        "      loss=one_training_step(model,images_batch,labels_batch)\n",
        "      if batch_counter % 100 ==0:\n",
        "        print(f\"loss batch {batch_counter}:{loss:.2f}\")\n"
      ],
      "metadata": {
        "id": "VGBAW_mcy9ei"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images,train_labels),(test_images,test_labels)=mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "\n",
        "fit(model,train_images,train_labels,epochs=10,batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5n9oxYp6pNs",
        "outputId": "7d897924-34f4-4f81-b366-99cf787b2550"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "loss batch 0:6.01\n",
            "loss batch 100:2.21\n",
            "loss batch 200:2.17\n",
            "loss batch 300:2.04\n",
            "loss batch 400:2.17\n",
            "Epoch 1\n",
            "loss batch 0:1.89\n",
            "loss batch 100:1.86\n",
            "loss batch 200:1.80\n",
            "loss batch 300:1.66\n",
            "loss batch 400:1.79\n",
            "Epoch 2\n",
            "loss batch 0:1.57\n",
            "loss batch 100:1.56\n",
            "loss batch 200:1.48\n",
            "loss batch 300:1.39\n",
            "loss batch 400:1.49\n",
            "Epoch 3\n",
            "loss batch 0:1.32\n",
            "loss batch 100:1.33\n",
            "loss batch 200:1.22\n",
            "loss batch 300:1.18\n",
            "loss batch 400:1.27\n",
            "Epoch 4\n",
            "loss batch 0:1.13\n",
            "loss batch 100:1.15\n",
            "loss batch 200:1.03\n",
            "loss batch 300:1.02\n",
            "loss batch 400:1.11\n",
            "Epoch 5\n",
            "loss batch 0:0.98\n",
            "loss batch 100:1.01\n",
            "loss batch 200:0.89\n",
            "loss batch 300:0.91\n",
            "loss batch 400:0.99\n",
            "Epoch 6\n",
            "loss batch 0:0.88\n",
            "loss batch 100:0.91\n",
            "loss batch 200:0.79\n",
            "loss batch 300:0.82\n",
            "loss batch 400:0.91\n",
            "Epoch 7\n",
            "loss batch 0:0.80\n",
            "loss batch 100:0.82\n",
            "loss batch 200:0.72\n",
            "loss batch 300:0.75\n",
            "loss batch 400:0.84\n",
            "Epoch 8\n",
            "loss batch 0:0.74\n",
            "loss batch 100:0.76\n",
            "loss batch 200:0.66\n",
            "loss batch 300:0.70\n",
            "loss batch 400:0.79\n",
            "Epoch 9\n",
            "loss batch 0:0.69\n",
            "loss batch 100:0.70\n",
            "loss batch 200:0.61\n",
            "loss batch 300:0.66\n",
            "loss batch 400:0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "predictions=model(test_images)\n",
        "predictions=predictions.numpy()\n",
        "\n",
        "predictions_labels=np.argmax(predictions,axis=1)\n",
        "matches = predictions_labels == test_labels\n",
        "print(f\"acc:{matches.mean():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU2MoHet3Q7s",
        "outputId": "93727b4a-ebc5-410c-ee40-13c47d742cdf"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc:0.82\n"
          ]
        }
      ]
    }
  ]
}